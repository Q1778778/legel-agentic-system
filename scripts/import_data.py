#!/usr/bin/env python3
"""
Data import system for Court Argument Simulator.
Creates sample Chinese legal cases and loads them into GraphRAG databases.
"""

import asyncio
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys
import random
import logging

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

from src.core.config import settings
from src.db.vector_db import VectorDB
from src.db.graph_db import GraphDB
from src.services.embeddings import EmbeddingService
from src.models.schemas import (
    ArgumentBundle,
    Case,
    Lawyer,
    Judge,
    Issue,
    Citation,
    ArgumentSegment,
    ConfidenceScore,
    GraphExplanation,
    StageType,
    DispositionType,
    RoleType,
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChineseLegalDataGenerator:
    """Generate realistic Chinese legal case data."""
    
    def __init__(self):
        # Initialize basic data first
        self.courts = [
            "åŒ—äº¬å¸‚ç¬¬ä¸€ä¸­çº§äººæ°‘æ³•é™¢", "ä¸Šæµ·å¸‚é«˜çº§äººæ°‘æ³•é™¢", "å¹¿å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
            "æ·±åœ³å¸‚ä¸­çº§äººæ°‘æ³•é™¢", "æ­å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢", "å—äº¬å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
            "æˆéƒ½å¸‚ä¸­çº§äººæ°‘æ³•é™¢", "æ­¦æ±‰å¸‚ä¸­çº§äººæ°‘æ³•é™¢", "è¥¿å®‰å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
            "æœ€é«˜äººæ°‘æ³•é™¢", "å¤©æ´¥å¸‚é«˜çº§äººæ°‘æ³•é™¢", "é‡åº†å¸‚é«˜çº§äººæ°‘æ³•é™¢"
        ]
        self.jurisdictions = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿ä¸œ", "æµ™æ±Ÿ", "æ±Ÿè‹", "å››å·", "æ¹–åŒ—", "é™•è¥¿", "å¤©æ´¥", "é‡åº†"]
        
        # Then initialize pools that depend on the basic data
        self.lawyers_pool = self._generate_lawyers()
        self.judges_pool = self._generate_judges()
        self.issues_pool = self._generate_issues()
    
    def _generate_lawyers(self) -> List[Dict[str, Any]]:
        """Generate pool of lawyers."""
        lawyers = []
        law_firms = [
            "é‡‘æœå¾‹å¸ˆäº‹åŠ¡æ‰€", "å›åˆå¾‹å¸ˆäº‹åŠ¡æ‰€", "ä¸­ä¼¦å¾‹å¸ˆäº‹åŠ¡æ‰€", "æµ·é—®å¾‹å¸ˆäº‹åŠ¡æ‰€",
            "æ–¹è¾¾å¾‹å¸ˆäº‹åŠ¡æ‰€", "é”¦å¤©åŸå¾‹å¸ˆäº‹åŠ¡æ‰€", "å¾·æ’å¾‹å¸ˆäº‹åŠ¡æ‰€", "ç›ˆç§‘å¾‹å¸ˆäº‹åŠ¡æ‰€",
            "å¤§æˆå¾‹å¸ˆäº‹åŠ¡æ‰€", "åº·è¾¾å¾‹å¸ˆäº‹åŠ¡æ‰€", "ç¯çƒå¾‹å¸ˆäº‹åŠ¡æ‰€", "é€šå•†å¾‹å¸ˆäº‹åŠ¡æ‰€"
        ]
        
        names = [
            "å¼ ä¼Ÿ", "æå¨œ", "ç‹å¼º", "åˆ˜æ•", "é™ˆæ°", "æ¨é™", "èµµç£Š", "å­™ä¸½",
            "å‘¨å‹‡", "å´çº¢", "å¾æ˜", "æœ±å", "èƒ¡äº®", "é«˜å³°", "æ—é›ª", "ä½•å†›",
            "éƒ­æ¶›", "é©¬è¶…", "éŸ©å†°", "æ›¹é˜³", "ç”°é‡", "çŸ³ç£Š", "å¤é›¨", "ç§¦é£",
            "æ˜“å»ºè”", "è´ºæ˜", "è°¢éœ†é”‹", "è‘£å¿", "æ±ªæ¶µ", "æä½³ç¦"
        ]
        
        for i, name in enumerate(names):
            lawyers.append({
                "id": f"lawyer_{i+1:03d}",
                "name": name,
                "bar_id": f"BJ{random.randint(100000, 999999)}",
                "firm": random.choice(law_firms)
            })
        
        return lawyers
    
    def _generate_judges(self) -> List[Dict[str, Any]]:
        """Generate pool of judges."""
        judges = []
        judge_names = [
            "å¼ å»ºå›½", "ææ˜å", "ç‹å¾·ä¼Ÿ", "åˆ˜å¿—å¼º", "é™ˆå»ºå", "æ¨ç«‹æ–°", "èµµæ˜¥æ¢…", "å­™å»ºå†›",
            "å‘¨ç«‹æ³¢", "å´å›½åº†", "å¾å¿—è¿œ", "æœ±å»ºå", "èƒ¡é”¦æ¶›", "é«˜å»ºå", "æ—å¿—ç²", "ä½•å»ºå›½",
            "éƒ­å¾·çº²", "é©¬åŒ–è…¾", "éŸ©çº¢æ¢…", "æ›¹å¿—æ˜", "ç”°å»ºå", "çŸ³å›½å¼º", "å¤å¿—å", "ç§¦å»ºå›½"
        ]
        
        for i, name in enumerate(judge_names):
            judges.append({
                "id": f"judge_{i+1:03d}",
                "name": f"{name}æ³•å®˜",
                "court": random.choice(self.courts)
            })
        
        return judges
    
    def _generate_issues(self) -> List[Dict[str, Any]]:
        """Generate pool of legal issues."""
        issues = [
            # Contract Law Issues
            {
                "id": "issue_contract_001",
                "title": "åˆåŒè¿çº¦æŸå®³èµ”å¿",
                "taxonomy_path": ["æ°‘æ³•", "åˆåŒæ³•", "è¿çº¦è´£ä»»", "æŸå®³èµ”å¿"]
            },
            {
                "id": "issue_contract_002",
                "title": "åˆåŒè§£é™¤æ¡ä»¶",
                "taxonomy_path": ["æ°‘æ³•", "åˆåŒæ³•", "åˆåŒè§£é™¤"]
            },
            {
                "id": "issue_contract_003",
                "title": "æ ¼å¼æ¡æ¬¾æ•ˆåŠ›",
                "taxonomy_path": ["æ°‘æ³•", "åˆåŒæ³•", "æ ¼å¼æ¡æ¬¾"]
            },
            {
                "id": "issue_contract_004",
                "title": "åˆåŒå±¥è¡ŒæŠ—è¾©æƒ",
                "taxonomy_path": ["æ°‘æ³•", "åˆåŒæ³•", "åˆåŒå±¥è¡Œ", "æŠ—è¾©æƒ"]
            },
            
            # Intellectual Property Issues
            {
                "id": "issue_ip_001",
                "title": "å•†æ ‡ä¾µæƒè®¤å®š",
                "taxonomy_path": ["çŸ¥è¯†äº§æƒæ³•", "å•†æ ‡æ³•", "å•†æ ‡ä¾µæƒ"]
            },
            {
                "id": "issue_ip_002",
                "title": "ä¸“åˆ©æƒä¿æŠ¤èŒƒå›´",
                "taxonomy_path": ["çŸ¥è¯†äº§æƒæ³•", "ä¸“åˆ©æ³•", "ä¸“åˆ©æƒä¿æŠ¤"]
            },
            {
                "id": "issue_ip_003",
                "title": "è‘—ä½œæƒåˆç†ä½¿ç”¨",
                "taxonomy_path": ["çŸ¥è¯†äº§æƒæ³•", "è‘—ä½œæƒæ³•", "åˆç†ä½¿ç”¨"]
            },
            {
                "id": "issue_ip_004",
                "title": "ä¸æ­£å½“ç«äº‰è¡Œä¸º",
                "taxonomy_path": ["çŸ¥è¯†äº§æƒæ³•", "åä¸æ­£å½“ç«äº‰æ³•"]
            },
            
            # Labor Law Issues
            {
                "id": "issue_labor_001",
                "title": "åŠ³åŠ¨åˆåŒè§£é™¤",
                "taxonomy_path": ["åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "åˆåŒè§£é™¤"]
            },
            {
                "id": "issue_labor_002",
                "title": "åŠ ç­è´¹è®¡ç®—",
                "taxonomy_path": ["åŠ³åŠ¨æ³•", "å·¥èµ„å¾…é‡", "åŠ ç­è´¹"]
            },
            {
                "id": "issue_labor_003",
                "title": "å·¥ä¼¤è®¤å®š",
                "taxonomy_path": ["åŠ³åŠ¨æ³•", "å·¥ä¼¤ä¿é™©", "å·¥ä¼¤è®¤å®š"]
            },
            {
                "id": "issue_labor_004",
                "title": "ç»æµè¡¥å¿é‡‘",
                "taxonomy_path": ["åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒ", "ç»æµè¡¥å¿"]
            },
            
            # Criminal Law Issues
            {
                "id": "issue_criminal_001",
                "title": "æ•…æ„ä¼¤å®³ç½ªæ„æˆè¦ä»¶",
                "taxonomy_path": ["åˆ‘æ³•", "ä¾µçŠ¯äººèº«æƒåˆ©ç½ª", "æ•…æ„ä¼¤å®³ç½ª"]
            },
            {
                "id": "issue_criminal_002",
                "title": "ç›—çªƒç½ªæ•°é¢è®¤å®š",
                "taxonomy_path": ["åˆ‘æ³•", "ä¾µçŠ¯è´¢äº§ç½ª", "ç›—çªƒç½ª"]
            },
            {
                "id": "issue_criminal_003",
                "title": "æ­£å½“é˜²å«ç•Œé™",
                "taxonomy_path": ["åˆ‘æ³•", "çŠ¯ç½ªæ„æˆ", "æ­£å½“é˜²å«"]
            },
            {
                "id": "issue_criminal_004",
                "title": "å…±åŒçŠ¯ç½ªè®¤å®š",
                "taxonomy_path": ["åˆ‘æ³•", "çŠ¯ç½ªæ„æˆ", "å…±åŒçŠ¯ç½ª"]
            },
            
            # Property Rights Issues
            {
                "id": "issue_property_001",
                "title": "æˆ¿å±‹ä¹°å–åˆåŒçº çº·",
                "taxonomy_path": ["æ°‘æ³•", "ç‰©æƒæ³•", "æˆ¿å±‹ä¹°å–"]
            },
            {
                "id": "issue_property_002",
                "title": "ç‰©æƒç¡®è®¤",
                "taxonomy_path": ["æ°‘æ³•", "ç‰©æƒæ³•", "ç‰©æƒç¡®è®¤"]
            },
            {
                "id": "issue_property_003",
                "title": "ç›¸é‚»å…³ç³»çº çº·",
                "taxonomy_path": ["æ°‘æ³•", "ç‰©æƒæ³•", "ç›¸é‚»å…³ç³»"]
            },
            
            # Company Law Issues
            {
                "id": "issue_company_001",
                "title": "è‚¡ä¸œæƒç›Šä¿æŠ¤",
                "taxonomy_path": ["å•†æ³•", "å…¬å¸æ³•", "è‚¡ä¸œæƒç›Š"]
            },
            {
                "id": "issue_company_002",
                "title": "è‘£äº‹è´£ä»»è®¤å®š",
                "taxonomy_path": ["å•†æ³•", "å…¬å¸æ³•", "è‘£äº‹è´£ä»»"]
            },
            {
                "id": "issue_company_003",
                "title": "å…¬å¸è§£æ•£æ¸…ç®—",
                "taxonomy_path": ["å•†æ³•", "å…¬å¸æ³•", "å…¬å¸è§£æ•£"]
            }
        ]
        
        return issues

    def generate_case_data(self, num_cases: int = 30) -> List[Dict[str, Any]]:
        """Generate realistic Chinese legal case data."""
        cases = []
        
        case_templates = {
            "contract": {
                "captions": [
                    "{}æœ‰é™å…¬å¸è¯‰{}æœ‰é™å…¬å¸åˆåŒçº çº·æ¡ˆ",
                    "{}å…¬å¸ä¸{}å…¬å¸ä¹°å–åˆåŒäº‰è®®æ¡ˆ",
                    "{}é›†å›¢è¯‰{}ä¼ä¸šåˆåŒè¿çº¦æ¡ˆ",
                    "{}è´¸æ˜“å…¬å¸ä¸{}åˆ¶é€ å…¬å¸ä¾›è´§åˆåŒçº çº·æ¡ˆ"
                ],
                "outcomes": ["èƒœè¯‰", "è´¥è¯‰", "éƒ¨åˆ†èƒœè¯‰", "è°ƒè§£", "æ’¤è¯‰"],
                "arguments": [
                    "æ ¹æ®ã€ŠåˆåŒæ³•ã€‹ç¬¬107æ¡è§„å®šï¼Œå½“äº‹äººä¸€æ–¹ä¸å±¥è¡ŒåˆåŒä¹‰åŠ¡æˆ–è€…å±¥è¡ŒåˆåŒä¹‰åŠ¡ä¸ç¬¦åˆçº¦å®šçš„ï¼Œåº”å½“æ‰¿æ‹…ç»§ç»­å±¥è¡Œã€é‡‡å–è¡¥æ•‘æªæ–½æˆ–è€…èµ”å¿æŸå¤±ç­‰è¿çº¦è´£ä»»ã€‚æœ¬æ¡ˆä¸­ï¼Œè¢«å‘Šæ˜ç¡®è¡¨ç¤ºä¸å†å±¥è¡ŒåˆåŒï¼Œæ„æˆæ ¹æœ¬è¿çº¦ã€‚",
                    "ä¾æ®ã€ŠåˆåŒæ³•ã€‹ç¬¬94æ¡ï¼Œå½“äº‹äººä¸€æ–¹æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€çš„ï¼Œå¯¹æ–¹å¯ä»¥è§£é™¤åˆåŒï¼šï¼ˆä¸€ï¼‰å› ä¸å¯æŠ—åŠ›è‡´ä½¿ä¸èƒ½å®ç°åˆåŒç›®çš„ã€‚æœ¬æ¡ˆç–«æƒ…æœŸé—´çš„ç‰¹æ®Šæƒ…å†µæ„æˆä¸å¯æŠ—åŠ›ï¼Œåº”å½“å…é™¤è¿çº¦è´£ä»»ã€‚",
                    "æ ¹æ®ã€Šæ°‘æ³•å…¸ã€‹ç¬¬563æ¡è§„å®šï¼Œæœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€çš„ï¼Œå½“äº‹äººå¯ä»¥è§£é™¤åˆåŒã€‚è¢«å‘Šçš„è¡Œä¸ºå·²ç»ä¸¥é‡å½±å“äº†åˆåŒç›®çš„çš„å®ç°ï¼ŒåŸå‘Šæœ‰æƒè§£é™¤åˆåŒå¹¶è¦æ±‚èµ”å¿æŸå¤±ã€‚",
                    "ã€ŠåˆåŒæ³•ã€‹ç¬¬60æ¡è§„å®šï¼Œå½“äº‹äººåº”å½“æŒ‰ç…§çº¦å®šå…¨é¢å±¥è¡Œè‡ªå·±çš„ä¹‰åŠ¡ã€‚å½“äº‹äººåº”å½“éµå¾ªè¯šå®ä¿¡ç”¨åŸåˆ™ï¼Œæ ¹æ®åˆåŒçš„æ€§è´¨ã€ç›®çš„å’Œäº¤æ˜“ä¹ æƒ¯å±¥è¡Œé€šçŸ¥ã€ååŠ©ã€ä¿å¯†ç­‰ä¹‰åŠ¡ã€‚"
                ]
            },
            "ip": {
                "captions": [
                    "{}å…¬å¸è¯‰{}å…¬å¸å•†æ ‡ä¾µæƒçº çº·æ¡ˆ",
                    "{}ç§‘æŠ€ä¸{}ç½‘ç»œè‘—ä½œæƒä¾µæƒæ¡ˆ",
                    "{}ç ”ç©¶é™¢è¯‰{}å…¬å¸ä¸“åˆ©ä¾µæƒæ¡ˆ",
                    "{}å“ç‰Œä¸{}ä¼ä¸šä¸æ­£å½“ç«äº‰æ¡ˆ"
                ],
                "outcomes": ["åœæ­¢ä¾µæƒ", "èµ”å¿æŸå¤±", "é©³å›è¯‰è®¼è¯·æ±‚", "è°ƒè§£å’Œè§£"],
                "arguments": [
                    "æ ¹æ®ã€Šå•†æ ‡æ³•ã€‹ç¬¬57æ¡è§„å®šï¼Œæœªç»å•†æ ‡æ³¨å†Œäººçš„è®¸å¯ï¼Œåœ¨åŒä¸€ç§å•†å“ä¸Šä½¿ç”¨ä¸å…¶æ³¨å†Œå•†æ ‡ç›¸åŒçš„å•†æ ‡çš„ï¼Œå±äºä¾µçŠ¯æ³¨å†Œå•†æ ‡ä¸“ç”¨æƒã€‚è¢«å‘Šçš„è¡Œä¸ºæ„æˆç›´æ¥ä¾µæƒã€‚",
                    "ä¾æ®ã€Šè‘—ä½œæƒæ³•ã€‹ç¬¬22æ¡ï¼Œåœ¨ä¸‹åˆ—æƒ…å†µä¸‹ä½¿ç”¨ä½œå“ï¼Œå¯ä»¥ä¸ç»è‘—ä½œæƒäººè®¸å¯ï¼Œä¸å‘å…¶æ”¯ä»˜æŠ¥é…¬ï¼Œä½†åº”å½“æŒ‡æ˜ä½œè€…å§“åã€ä½œå“åç§°ã€‚è¢«å‘Šä½¿ç”¨æ„æˆåˆç†ä½¿ç”¨ã€‚",
                    "ã€Šä¸“åˆ©æ³•ã€‹ç¬¬11æ¡è§„å®šï¼Œå‘æ˜å’Œå®ç”¨æ–°å‹ä¸“åˆ©æƒè¢«æˆäºˆåï¼Œé™¤æœ¬æ³•å¦æœ‰è§„å®šçš„ä»¥å¤–ï¼Œä»»ä½•å•ä½æˆ–è€…ä¸ªäººæœªç»ä¸“åˆ©æƒäººè®¸å¯ï¼Œéƒ½ä¸å¾—å®æ–½å…¶ä¸“åˆ©ã€‚è¢«å‘Šäº§å“è½å…¥åŸå‘Šä¸“åˆ©æƒä¿æŠ¤èŒƒå›´ã€‚",
                    "æ ¹æ®ã€Šåä¸æ­£å½“ç«äº‰æ³•ã€‹ç¬¬2æ¡ï¼Œç»è¥è€…åœ¨ç”Ÿäº§ç»è¥æ´»åŠ¨ä¸­ï¼Œåº”å½“éµå¾ªè‡ªæ„¿ã€å¹³ç­‰ã€å…¬å¹³ã€è¯šä¿¡çš„åŸåˆ™ï¼Œéµå®ˆæ³•å¾‹å’Œå•†ä¸šé“å¾·ã€‚è¢«å‘Šè¡Œä¸ºè¿åè¯šå®ä¿¡ç”¨åŸåˆ™ã€‚"
                ]
            },
            "labor": {
                "captions": [
                    "{}è¯‰{}å…¬å¸åŠ³åŠ¨äº‰è®®æ¡ˆ",
                    "{}ä¸{}é›†å›¢åŠ³åŠ¨åˆåŒçº çº·æ¡ˆ",
                    "{}è¯‰{}ä¼ä¸šå·¥ä¼¤èµ”å¿æ¡ˆ",
                    "{}ä¸{}å…¬å¸åŠ ç­è´¹äº‰è®®æ¡ˆ"
                ],
                "outcomes": ["æ”¯æŒåŠ³åŠ¨è€…", "æ”¯æŒç”¨äººå•ä½", "éƒ¨åˆ†æ”¯æŒ", "è°ƒè§£ç»“æ¡ˆ"],
                "arguments": [
                    "æ ¹æ®ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬39æ¡è§„å®šï¼ŒåŠ³åŠ¨è€…æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€çš„ï¼Œç”¨äººå•ä½å¯ä»¥è§£é™¤åŠ³åŠ¨åˆåŒã€‚ä½†ç”¨äººå•ä½åº”å½“å±¥è¡Œä¸¾è¯è´£ä»»ï¼Œè¯æ˜åŠ³åŠ¨è€…å­˜åœ¨ä¸¥é‡è¿çºªè¡Œä¸ºã€‚",
                    "ã€ŠåŠ³åŠ¨æ³•ã€‹ç¬¬44æ¡è§„å®šï¼Œæœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€çš„ï¼Œç”¨äººå•ä½åº”å½“æŒ‰ç…§ä¸‹åˆ—æ ‡å‡†æ”¯ä»˜é«˜äºåŠ³åŠ¨è€…æ­£å¸¸å·¥ä½œæ—¶é—´å·¥èµ„çš„å·¥èµ„æŠ¥é…¬ï¼šï¼ˆä¸€ï¼‰å®‰æ’åŠ³åŠ¨è€…å»¶é•¿æ—¶é—´çš„ï¼Œæ”¯ä»˜ä¸ä½äºå·¥èµ„çš„ç™¾åˆ†ä¹‹ä¸€ç™¾äº”åçš„å·¥èµ„æŠ¥é…¬ã€‚",
                    "ä¾æ®ã€Šå·¥ä¼¤ä¿é™©æ¡ä¾‹ã€‹ç¬¬14æ¡ï¼ŒèŒå·¥æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€çš„ï¼Œåº”å½“è®¤å®šä¸ºå·¥ä¼¤ã€‚æœ¬æ¡ˆäº‹æ•…å‘ç”Ÿåœ¨å·¥ä½œæ—¶é—´å’Œå·¥ä½œåœºæ‰€å†…ï¼Œç¬¦åˆå·¥ä¼¤è®¤å®šæ¡ä»¶ã€‚",
                    "ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹ç¬¬47æ¡è§„å®šï¼Œç»æµè¡¥å¿æŒ‰åŠ³åŠ¨è€…åœ¨æœ¬å•ä½å·¥ä½œçš„å¹´é™ï¼Œæ¯æ»¡ä¸€å¹´æ”¯ä»˜ä¸€ä¸ªæœˆå·¥èµ„çš„æ ‡å‡†å‘åŠ³åŠ¨è€…æ”¯ä»˜ã€‚è¿æ³•è§£é™¤åº”æ”¯ä»˜äºŒå€ç»æµè¡¥å¿ã€‚"
                ]
            },
            "criminal": {
                "captions": [
                    "äººæ°‘æ£€å¯Ÿé™¢è¯‰{}æ•…æ„ä¼¤å®³æ¡ˆ",
                    "äººæ°‘æ£€å¯Ÿé™¢è¯‰{}ç›—çªƒæ¡ˆ",
                    "äººæ°‘æ£€å¯Ÿé™¢è¯‰{}è¯ˆéª—æ¡ˆ",
                    "äººæ°‘æ£€å¯Ÿé™¢è¯‰{}å±é™©é©¾é©¶æ¡ˆ"
                ],
                "outcomes": ["æœ‰ç½ª", "æ— ç½ª", "å…äºˆåˆ‘äº‹å¤„ç½š", "ç¼“åˆ‘"],
                "arguments": [
                    "æ ¹æ®ã€Šåˆ‘æ³•ã€‹ç¬¬234æ¡è§„å®šï¼Œæ•…æ„ä¼¤å®³ä»–äººèº«ä½“çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚è¢«å‘Šäººçš„è¡Œä¸ºç¬¦åˆæ•…æ„ä¼¤å®³ç½ªçš„æ„æˆè¦ä»¶ï¼Œåº”å½“ä¾æ³•æ‰¿æ‹…åˆ‘äº‹è´£ä»»ã€‚",
                    "ä¾æ®ã€Šåˆ‘æ³•ã€‹ç¬¬20æ¡ï¼Œä¸ºäº†ä½¿å›½å®¶ã€å…¬å…±åˆ©ç›Šã€æœ¬äººæˆ–è€…ä»–äººçš„äººèº«ã€è´¢äº§å’Œå…¶ä»–æƒåˆ©å…å—æ­£åœ¨è¿›è¡Œçš„ä¸æ³•ä¾µå®³ï¼Œè€Œé‡‡å–çš„åˆ¶æ­¢ä¸æ³•ä¾µå®³çš„è¡Œä¸ºï¼Œå¯¹ä¸æ³•ä¾µå®³äººé€ æˆæŸå®³çš„ï¼Œå±äºæ­£å½“é˜²å«ã€‚",
                    "ã€Šåˆ‘æ³•ã€‹ç¬¬264æ¡è§„å®šï¼Œç›—çªƒå…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œæˆ–è€…å¤šæ¬¡ç›—çªƒã€å…¥æˆ·ç›—çªƒã€æºå¸¦å‡¶å™¨ç›—çªƒã€æ‰’çªƒçš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚è¢«å‘Šè¡Œä¸ºæ„æˆç›—çªƒç½ªã€‚",
                    "æ ¹æ®ã€Šåˆ‘äº‹è¯‰è®¼æ³•ã€‹ç¬¬195æ¡ï¼Œåœ¨è¢«å‘Šäººæœ€åé™ˆè¿°åï¼Œå®¡åˆ¤é•¿å®£å¸ƒä¼‘åº­ï¼Œåˆè®®åº­è¿›è¡Œè¯„è®®ï¼Œæ ¹æ®å·²ç»æŸ¥æ˜çš„äº‹å®ã€è¯æ®å’Œæœ‰å…³çš„æ³•å¾‹è§„å®šï¼Œåˆ†åˆ«ä½œå‡ºåˆ¤å†³ã€‚"
                ]
            }
        }
        
        companies = ["åä¸º", "è…¾è®¯", "é˜¿é‡Œå·´å·´", "ç™¾åº¦", "å°ç±³", "å­—èŠ‚è·³åŠ¨", "ç¾å›¢", "æ»´æ»´", "äº¬ä¸œ", "ç½‘æ˜“", "æ–°æµª", "æœç‹"]
        people_names = ["å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­", "åˆ˜ä¸ƒ", "é™ˆå…«", "æ¨ä¹", "å­™å"]
        
        for i in range(num_cases):
            case_type = random.choice(list(case_templates.keys()))
            template = case_templates[case_type]
            
            # Generate case details
            if case_type == "criminal":
                plaintiff = "åŒ—äº¬å¸‚äººæ°‘æ£€å¯Ÿé™¢"
                defendant = random.choice(people_names)
                caption = random.choice(template["captions"]).format(defendant)
            else:
                plaintiff = random.choice(companies)
                defendant = random.choice(companies)
                while defendant == plaintiff:
                    defendant = random.choice(companies)
                caption = random.choice(template["captions"]).format(plaintiff, defendant)
            
            # Select related issue
            issue_candidates = [issue for issue in self.issues_pool 
                             if case_type in issue["taxonomy_path"][0].lower() or 
                                case_type in issue["id"]]
            if not issue_candidates:
                issue_candidates = self.issues_pool[:5]  # Fallback
            
            issue = random.choice(issue_candidates)
            lawyer = random.choice(self.lawyers_pool)
            judge = random.choice(self.judges_pool)
            
            case_data = {
                "case": {
                    "id": f"case_{i+1:03d}",
                    "caption": caption,
                    "court": judge["court"],
                    "jurisdiction": random.choice(self.jurisdictions),
                    "judge_id": judge["id"],
                    "judge_name": judge["name"],
                    "filed_date": self._random_date(),
                    "outcome": random.choice(template["outcomes"])
                },
                "lawyer": lawyer,
                "judge": judge,
                "issue": issue,
                "argument_text": random.choice(template["arguments"]),
                "case_type": case_type
            }
            
            cases.append(case_data)
        
        return cases
    
    def _random_date(self) -> datetime:
        """Generate random date within the last 5 years."""
        start_date = datetime.now() - timedelta(days=5*365)
        end_date = datetime.now() - timedelta(days=30)
        random_days = random.randint(0, (end_date - start_date).days)
        return start_date + timedelta(days=random_days)


class DataImporter:
    """Import generated legal data into GraphRAG databases."""
    
    def __init__(self):
        self.vector_db = VectorDB()
        self.graph_db = GraphDB()
        self.embedding_service = EmbeddingService()
        self.generator = ChineseLegalDataGenerator()
    
    async def import_all_data(self, num_cases: int = 30):
        """Import all generated data into databases."""
        logger.info(f"Starting import of {num_cases} legal cases...")
        
        # Generate case data
        logger.info("Generating case data...")
        case_data = self.generator.generate_case_data(num_cases)
        
        # Create argument bundles
        logger.info("Creating argument bundles...")
        bundles = []
        for case in case_data:
            bundle = await self._create_argument_bundle(case)
            if bundle:
                bundles.append(bundle)
        
        logger.info(f"Created {len(bundles)} argument bundles")
        
        # Import to graph database
        logger.info("Importing to graph database...")
        await self._import_to_graph(bundles)
        
        # Import to vector database
        logger.info("Importing to vector database...")
        await self._import_to_vector(bundles)
        
        logger.info(f"Successfully imported {len(bundles)} cases")
    
    async def _create_argument_bundle(self, case_data: Dict[str, Any]) -> Optional[ArgumentBundle]:
        """Create argument bundle from case data."""
        try:
            # Create models from data
            case = Case(**case_data["case"])
            lawyer = Lawyer(**case_data["lawyer"])
            issue = Issue(**case_data["issue"])
            
            # Create argument segments
            argument_id = f"arg_{uuid.uuid4().hex[:8]}"
            segments = await self._create_segments(argument_id, case_data["argument_text"])
            
            # Create citations (sample)
            citations = self._generate_citations(case_data["case_type"])
            
            # Create confidence score
            confidence = ConfidenceScore(
                value=random.uniform(0.7, 0.95),
                explanation=GraphExplanation(
                    graph_hops=["Issueâ†’Argument", "Argumentâ†’Case"],
                    boosts={"relevance": 0.1, "recency": 0.05},
                    final_score=random.uniform(0.7, 0.95)
                ),
                features={"vector_similarity": 0.8, "graph_connectivity": 0.2}
            )
            
            # Create bundle
            bundle = ArgumentBundle(
                argument_id=argument_id,
                confidence=confidence,
                case=case,
                lawyer=lawyer,
                issue=issue,
                stage=random.choice(list(StageType)),
                disposition=random.choice(list(DispositionType)),
                citations=citations,
                segments=segments,
                signature_hash=hashlib.md5(case_data["argument_text"].encode()).hexdigest(),
                tenant="default"
            )
            
            return bundle
            
        except Exception as e:
            logger.error(f"Error creating argument bundle: {e}")
            return None
    
    async def _create_segments(self, argument_id: str, text: str) -> List[ArgumentSegment]:
        """Create argument segments from text."""
        # Split text into logical segments
        sentences = text.split("ã€‚")
        segments = []
        
        for i, sentence in enumerate(sentences):
            if sentence.strip():
                segment = ArgumentSegment(
                    segment_id=f"seg_{argument_id}_{i+1:02d}",
                    argument_id=argument_id,
                    text=sentence.strip() + "ã€‚",
                    role=RoleType.OPENING if i == 0 else RoleType.ANSWER,
                    seq=i,
                    citations=self._extract_citations(sentence),
                    score=random.uniform(0.6, 0.9)
                )
                segments.append(segment)
        
        return segments
    
    def _extract_citations(self, text: str) -> List[str]:
        """Extract citations from text (simplified)."""
        citations = []
        # Look for law references
        if "ã€Š" in text and "ã€‹" in text:
            start = text.find("ã€Š")
            end = text.find("ã€‹", start)
            if end > start:
                law_ref = text[start:end+1]
                citations.append(law_ref)
        
        # Look for article references
        if "ç¬¬" in text and "æ¡" in text:
            import re
            pattern = r'ç¬¬\d+æ¡'
            matches = re.findall(pattern, text)
            citations.extend(matches)
        
        return citations
    
    def _generate_citations(self, case_type: str) -> List[Citation]:
        """Generate relevant citations based on case type."""
        citation_map = {
            "contract": [
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹", type="statute"),
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•ã€‹", type="statute"),
                Citation(text="æœ€é«˜äººæ°‘æ³•é™¢å…³äºé€‚ç”¨ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•ã€‹è‹¥å¹²é—®é¢˜çš„è§£é‡Š(ä¸€)", type="regulation"),
            ],
            "ip": [
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½å•†æ ‡æ³•ã€‹", type="statute"),
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½ä¸“åˆ©æ³•ã€‹", type="statute"),
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½è‘—ä½œæƒæ³•ã€‹", type="statute"),
            ],
            "labor": [
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨æ³•ã€‹", type="statute"),
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•ã€‹", type="statute"),
                Citation(text="ã€Šå·¥ä¼¤ä¿é™©æ¡ä¾‹ã€‹", type="regulation"),
            ],
            "criminal": [
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹", type="statute"),
                Citation(text="ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘äº‹è¯‰è®¼æ³•ã€‹", type="statute"),
            ]
        }
        
        return citation_map.get(case_type, [])
    
    async def _import_to_graph(self, bundles: List[ArgumentBundle]):
        """Import bundles to graph database."""
        for i, bundle in enumerate(bundles):
            try:
                success = self.graph_db.upsert_nodes_and_edges(bundle, tenant="default")
                if success:
                    logger.info(f"Imported bundle {i+1}/{len(bundles)} to graph DB")
                else:
                    logger.warning(f"Failed to import bundle {bundle.argument_id} to graph DB")
            except Exception as e:
                logger.error(f"Error importing bundle {bundle.argument_id} to graph: {e}")
    
    async def _import_to_vector(self, bundles: List[ArgumentBundle]):
        """Import bundles to vector database."""
        for i, bundle in enumerate(bundles):
            try:
                # Generate embeddings for segments
                segment_texts = [seg.text for seg in bundle.segments]
                embeddings = await self.embedding_service.embed_batch(segment_texts)
                
                # Create metadata
                metadata = {
                    "tenant": bundle.tenant,
                    "lawyer": {
                        "id": bundle.lawyer.id if bundle.lawyer else None,
                        "name": bundle.lawyer.name if bundle.lawyer else None,
                        "firm": bundle.lawyer.firm if bundle.lawyer else None,
                    },
                    "case": {
                        "id": bundle.case.id,
                        "caption": bundle.case.caption,
                        "court": bundle.case.court,
                        "jurisdiction": bundle.case.jurisdiction,
                        "judge_id": bundle.case.judge_id,
                        "judge_name": bundle.case.judge_name,
                        "filed_date": bundle.case.filed_date.isoformat() if bundle.case.filed_date else None,
                        "outcome": bundle.case.outcome,
                    },
                    "issue": {
                        "id": bundle.issue.id,
                        "title": bundle.issue.title,
                        "taxonomy_path": bundle.issue.taxonomy_path,
                    },
                    "stage": bundle.stage.value if bundle.stage else None,
                    "disposition": bundle.disposition.value if bundle.disposition else None,
                    "filed_year": bundle.case.filed_date.year if bundle.case.filed_date else None,
                    "signature_hash": bundle.signature_hash,
                    "src": "generated_chinese_legal_data"
                }
                
                # Upload to vector DB
                success = self.vector_db.upsert_segments(bundle.segments, embeddings, metadata)
                if success:
                    logger.info(f"Imported bundle {i+1}/{len(bundles)} to vector DB")
                else:
                    logger.warning(f"Failed to import bundle {bundle.argument_id} to vector DB")
                    
            except Exception as e:
                logger.error(f"Error importing bundle {bundle.argument_id} to vector: {e}")


async def main():
    """Main import function."""
    print("ğŸ›ï¸  Court Argument Simulator - Data Import System")
    print("="*60)
    
    try:
        # Initialize importer
        importer = DataImporter()
        
        # Check database connections
        print("Checking database connections...")
        
        # Test vector DB
        try:
            info = importer.vector_db.get_collection_info()
            print(f"âœ… Vector DB connected - Collection: {info['name']}, Vector size: {info['vector_size']}")
        except Exception as e:
            print(f"âŒ Vector DB connection failed: {e}")
            return
        
        # Test graph DB
        try:
            with importer.graph_db.driver.session() as session:
                result = session.run("RETURN 'Graph DB Connected' as status")
                print(f"âœ… Graph DB connected - {result.single()['status']}")
        except Exception as e:
            print(f"âŒ Graph DB connection failed: {e}")
            return
        
        # Import data
        num_cases = 30
        print(f"\nImporting {num_cases} Chinese legal cases...")
        await importer.import_all_data(num_cases)
        
        print("\nâœ… Data import completed successfully!")
        
        # Show summary
        try:
            vector_info = importer.vector_db.get_collection_info()
            print(f"\nğŸ“Š Import Summary:")
            print(f"   Vector DB points: {vector_info['points_count']}")
            print(f"   Collection status: {vector_info['status']}")
            
            with importer.graph_db.driver.session() as session:
                result = session.run("""
                    MATCH (n) 
                    RETURN 
                        count(DISTINCT n) as total_nodes,
                        count(DISTINCT CASE WHEN 'Case' IN labels(n) THEN n END) as cases,
                        count(DISTINCT CASE WHEN 'Argument' IN labels(n) THEN n END) as arguments,
                        count(DISTINCT CASE WHEN 'Lawyer' IN labels(n) THEN n END) as lawyers,
                        count(DISTINCT CASE WHEN 'Judge' IN labels(n) THEN n END) as judges,
                        count(DISTINCT CASE WHEN 'Issue' IN labels(n) THEN n END) as issues
                """)
                stats = result.single()
                print(f"   Graph DB nodes: {stats['total_nodes']}")
                print(f"   Cases: {stats['cases']}, Arguments: {stats['arguments']}")
                print(f"   Lawyers: {stats['lawyers']}, Judges: {stats['judges']}, Issues: {stats['issues']}")
                
        except Exception as e:
            logger.error(f"Error getting summary stats: {e}")
        
    except Exception as e:
        logger.error(f"Import failed: {e}")
        raise
    finally:
        # Cleanup
        if 'importer' in locals():
            importer.graph_db.close()


if __name__ == "__main__":
    asyncio.run(main())