# MCP Configuration File
# Production configuration for MCP server integration

# MCP Servers Configuration
servers:
  case_extractor:
    name: "case_extractor"
    type: "case_extractor"
    command:
      - "python"
      - "-m"
      - "mcp_case_extractor.server"
    args: []
    env:
      PYTHONPATH: "."
      LOG_LEVEL: "INFO"
    working_dir: "/Users/carpewang/Desktop/bili/legel-agentic-system"
    auto_restart: true
    max_retries: 5
    retry_delay: 1.0
    health_check_interval: 30.0
    timeout: 60.0
    
  lawyer_server:
    name: "lawyer_server"
    type: "lawyer_server"
    command:
      - "python"
      - "-m"
      - "mcp_lawyer_server.server"
    args: []
    env:
      PYTHONPATH: "."
      LOG_LEVEL: "INFO"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
    working_dir: "/Users/carpewang/Desktop/bili/legel-agentic-system"
    auto_restart: true
    max_retries: 5
    retry_delay: 1.0
    health_check_interval: 30.0
    timeout: 60.0

# Session Management Configuration
session:
  storage_backend: "memory"  # Options: memory, redis
  redis_url: "redis://localhost:6379"
  session_timeout: 3600  # 1 hour
  max_sessions_per_client: 10
  cleanup_interval: 300  # 5 minutes

# WebSocket Configuration
websocket:
  ping_interval: 30  # seconds
  ping_timeout: 60  # seconds
  max_message_size: 10485760  # 10MB
  max_connections_per_client: 5

# MCP Bridge Configuration
bridge:
  connection_timeout: 30  # seconds
  request_timeout: 60  # seconds
  max_concurrent_requests: 100
  enable_connection_pooling: true
  pool_size: 10

# Logging Configuration
logging:
  level: "INFO"
  format: "json"
  file: "logs/mcp.log"
  max_size: "100MB"
  max_files: 10
  enable_console: true

# Monitoring Configuration
monitoring:
  enable_metrics: true
  metrics_port: 9090
  enable_health_check: true
  health_check_port: 8081

# Security Configuration
security:
  enable_authentication: false
  jwt_secret: "${JWT_SECRET}"
  api_key_header: "X-API-Key"
  allowed_origins:
    - "http://localhost:3000"
    - "http://localhost:8501"
    - "http://localhost:8502"
  enable_rate_limiting: true
  rate_limit_requests: 100
  rate_limit_window: 60  # seconds

# Feature Flags
features:
  enable_case_extraction: true
  enable_legal_consultation: true
  enable_opponent_simulation: true
  enable_streaming: true
  enable_file_upload: true
  max_file_size: 52428800  # 50MB

# Integration Configuration
integration:
  graphrag:
    enabled: true
    endpoint: "http://localhost:8000/api/v1"
    timeout: 30
  
  vector_db:
    enabled: true
    endpoint: "http://localhost:8000/api/v1"
    
  llm:
    provider: "openai"
    model: "gpt-4-turbo-preview"
    temperature: 0.7
    max_tokens: 4096
    streaming: true

# Development Configuration
development:
  debug: false
  reload: false
  cors_allow_all: false
  mock_mode: false

# Production Configuration
production:
  ssl_enabled: false
  ssl_cert_file: "/path/to/cert.pem"
  ssl_key_file: "/path/to/key.pem"
  workers: 4
  worker_class: "uvicorn.workers.UvicornWorker"